# ML_search_data_in_inet
## Методы сбора и обработки данных из сети Интернет
### Домашнее задание к Уроку 6
#### Фреймворк Scrapy, pipelines, Splash
Стек: ML: scrapy, pymongo

Задача:

#### Вариант I

1) Доработать паука в имеющемся проекте, чтобы он формировал item по структуре:

    *Наименование вакансии

    *Зарплата от

    *Зарплата до
 
    *Ссылку на саму вакансию

И складывал все записи в БД(любую)

2) Создать в имеющемся проекте второго паука по сбору вакансий с сайта superjob. 
Паук должен формировать item'ы по аналогичной структуре и складывать данные также в БД

### Вариант II

1) Создать пауков по сбору данных о книгах с сайтов labirint.ru и/или book24.ru

2) Каждый паук должен собирать:

   * Ссылку на книгу
   * Наименование книги
   * Автор(ы)
   * Основную цену
   * Цену со скидкой
   * Рейтинг книги
   
3) Собранная информация должна складываться в базу данных

#### Описание решения 

Для реализации был выбрано задание из Варианта II. Для реализации парсинга был выбран сайт labirint.ru. 
На сайте выполняем парсинг всех книг из раздела Новинки.

Из данного блока забираем все ссылки на страницы всех представленных книг, и затем парсим с каждой страницы книги 
следующие атрибуты:

Наименование книги, cсылку на страницу книги, ссылка на фото (загружаем и сохраняем на локальный диск), 
два вида цены (стандартную и со скидкой), ФИО авторов, ФИО переводчиков, код ISBN книги, количество страниц и 
рейтинг книги.

Затем записываем все полученные данные по книги в документальную БД - MongoDB развернутую локально, в том числе и 
мета-данне по сохраненной фотографии обложки (хеш, ссылку, путь и имя на локальном диске, статус (скачено/обновлено)) 

В настройках включены и передаются coociеs сайта.

Скриншот MongoDB Compass с записанными данными по товарам находятся в каталоге Lesson6/pics/.

Так же фото обложек хранятся в каталоге parser_books/photo (Все скаченные картинки не выкладываю только несколько).

Информацию собранную с сайта так же можно найти в файле /parser_books/parser_books.log

Scrapy проект находиться в каталоге Lesson5/parser_books 

Структура проекта стандартная. 

Используются следующие элементы:

1. Паук spiders/labirint_ru.py 
2. items.py
3. pipeline.py

Все настройки scrapy в файле setting.py

Для запуска: 

   /parser_books/scrapy crawl labirint_ru