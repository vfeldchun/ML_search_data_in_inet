# ML_search_data_in_inet
## Методы сбора и обработки данных из сети Интернет
### Домашнее задание к Уроку 6
#### Парсинг данных. Selenium в Python
#### Стек: ML: selenium, scrapy, pymongo, pillow, time

Задача:

1) Взять любую категорию товаров на сайте Леруа Мерлен (если не работает - возьмите его аналог castorama) 
Собрать следующие данные:

    ● название;
    ● все фото;
    ● ссылка;
    ● цена.

Реализуйте очистку и преобразование данных с помощью ItemLoader. Цены должны быть в виде числового значения.

#### Описание решения

Для реализации парсинга был выбран сайт castorama.ru. На сайте выполняем парсинг всех товаров из раздела Перфораторы.

Из данного блока забираем все товары, и затем парсим атрибуты каждого товра:

Наименование товара, cсылку на страницу товара, ссылка на фото (загружаем и сохраняем на локальный диск), 
два вида цены (текущую и старую если есть).

Затем записываем все полученные данные по товару в документальную БД - MongoDB развернутую локально, 
в том числе и мета-данне по сохраненной фотографии товара (хеш, ссылку, путь и имя на локальном диске, 
статус (скачено/обновлено))

Скриншот MongoDB Compass с записанными данными по товарам находятся в каталоге Lesson7/parser_goods/pics/.

Так же фото в каталоге parser_goods/photo.

Информацию собранную с сайта так же можно найти в файле /parser_goods/parser_goods.log

Scrapy проект находиться в каталоге Lesson7/parser_goods

Структура проекта стандартная.

Используются следующие элементы:

1. Паук spiders/castorama.py
2. items.py
3. pipeline.py
4. Все настройки scrapy в файле setting.py
5. Selenium webdriver для chrome находится в папке parser_goods/chrome_driver

Для запуска:

/parser_goods/scrapy crawl castorama

** Нужно чтобы путь до вэбдрайвера был в переменной окружения PATH