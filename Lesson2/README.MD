# Методы сбора и обработки данных из сети Интернет
## Домашнее задание к Уроку 2
### Парсинг данных. HTML, DOM, XPath

#### Стек:
ML: requests, lxml(html), datetime, json, time, logging, pymongo

#### Задача:
Написать приложение и функцию, которые собирают основные новости с сайта на выбор lenta.ru, mail.ru, dzen.ru . Для парсинга использовать XPath
Структура данных должна содержать:
* название источника
* наименование новости
* ссылку на новость
* дата публикации

#### Описание решения
Написан код парсера который храниться в файле news_parser.py
В коде реализован парсинт новостей трех заданных сайтов (см. комментарии к коду).
Обработанные новости в соответствии с заданной структурой сохраняются в БД MongoDB newsdb,
соответственно в коллекции lenta, mail и dzen.
Так же результаты парсинга сохраняются в формате json в каталог jsons.
json файлы содержат данные всех результатов парсинга новостей за день по каждому новостному 
сайту в отдельном файле.

Ведется логирование работы парсера и сохраняется в /Loggs/news_parser.log.
Журнал ротируется 10 раз по достижению размера 97КБайт, потом начинается перезапись.

В каталоге YP находиться Yupiter Notebook (HW_Lesson2.ipynb) c информацией как проверялась 
работоспособность кода парсинга.

В каталоге /pics хранятся скриншоты их MangoDB Compass с данными по колекциям и документам
записанным парсером в БД.

#### Рекомендации по запуску
Рекомендуется настроить запуск каждые 30 или 60 минут по расписанию запуском команды:
$ python news_parser.py
из каталога в котором будет храниться парсер.

